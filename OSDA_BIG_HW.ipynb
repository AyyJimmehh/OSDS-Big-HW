{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc_ljZQrtS9b",
        "outputId": "3afb75ca-e895-4724-95ce-007d5f99e706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mmeZj002WOS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AndrewDiv/FCALC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMaI6-QveESz"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sYl6GTatdnK"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly2IZ8xP4gJH"
      },
      "outputs": [],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\")\n",
        "od.download(\"https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\")\n",
        "od.download(\"https://www.kaggle.com/datasets/adityakadiwal/water-potability\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyM1rWpouFEN"
      },
      "outputs": [],
      "source": [
        "file1 = ('/content/heart-attack-analysis-prediction-dataset/heart.csv')\n",
        "heart_df = pd.read_csv(file1)\n",
        "\n",
        "file2 = ('/content/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\n",
        "stroke_df = pd.read_csv(file2)\n",
        "\n",
        "file3 = ('/content/water-potability/water_potability.csv')\n",
        "water_df = pd.read_csv(file3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from FCALC import fcalc"
      ],
      "metadata": {
        "id": "HsxyGT8DMGhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfiNNtgrCnzB"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkEiBTyovE_1"
      },
      "outputs": [],
      "source": [
        "heart_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVh2cB1V1UB0"
      },
      "outputs": [],
      "source": [
        "heart_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80YhTIckvHQq"
      },
      "outputs": [],
      "source": [
        "stroke_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stroke_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "YGjzMwm0Xdcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGCHY08I1P96"
      },
      "outputs": [],
      "source": [
        "stroke_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h242mLK2eaN"
      },
      "outputs": [],
      "source": [
        "#Dealing with Categorical variables in stroke dataset\n",
        "stroke_df['bmi'] = stroke_df['bmi'].fillna(stroke_df['bmi'].mean())\n",
        "\n",
        "stroke_df['ever_married'] = stroke_df['ever_married'].replace(['No', 'Yes'], [0, 1])\n",
        "stroke_df['Residence_type'] = stroke_df['Residence_type'].replace(['Rural', 'Urban'], [0, 1])\n",
        "\n",
        "stroke_df = pd.get_dummies(stroke_df, columns=['gender', 'work_type', 'smoking_status'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strk = stroke_df.pop('stroke')"
      ],
      "metadata": {
        "id": "Tgu2fpqckMme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stroke_df.insert(19, 'stroke', strk)"
      ],
      "metadata": {
        "id": "y6Hs0uv-kUQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZrzTE7KvP2y"
      },
      "outputs": [],
      "source": [
        "water_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cQbIqqA1cb-"
      },
      "outputs": [],
      "source": [
        "water_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8x5xegE3KU8"
      },
      "outputs": [],
      "source": [
        "#Dealing with Categorical variables in water dataset\n",
        "water_df['ph'] = water_df['ph'].fillna(water_df['ph'].mean())\n",
        "water_df['Sulfate'] = water_df['Sulfate'].fillna(water_df['Sulfate'].mean())\n",
        "water_df['Trihalomethanes'] = water_df['Trihalomethanes'].fillna(water_df['Trihalomethanes'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpNoLU7hIx3T"
      },
      "source": [
        "# **Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3zOXs057yVz"
      },
      "outputs": [],
      "source": [
        "def tune_decision_tree(dataset, target_column_name):\n",
        "    # Split the data into features (x) and the target variable (y)\n",
        "    x = dataset.drop(columns=[target_column_name])\n",
        "    y = dataset[target_column_name]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a DecisionTreeClassifier\n",
        "    classifier = DecisionTreeClassifier()\n",
        "\n",
        "    # Define a parameter grid for tuning\n",
        "    param_grid = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Create a GridSearchCV object with 5-fold cross-validation\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Get the best parameters and estimator from the grid search\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    # Assess the model using 5-fold cross-validation and print accuracy and F1 scores\n",
        "    accuracy_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Cross-Validation Accuracy:\", round(np.mean(accuracy_scores),3))\n",
        "    print(\"Cross-Validation F1 Score:\", round(np.mean(f1_scores),3))\n",
        "\n",
        "    # Fit the best model on the entire training set\n",
        "    best_estimator.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = best_estimator.predict(x_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Test Set Accuracy:\", round(test_accuracy,3))\n",
        "    print(\"Test Set F1 Score:\", round(test_f1_score,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM12DcKf4IHb"
      },
      "outputs": [],
      "source": [
        "tune_decision_tree(heart_df, 'output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ZpI7yC9aY7"
      },
      "outputs": [],
      "source": [
        "tune_decision_tree(stroke_df, 'stroke')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gsI3RXZ_Paf"
      },
      "outputs": [],
      "source": [
        "tune_decision_tree(water_df, 'Potability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZPpMn7pI0jY"
      },
      "source": [
        "# **Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue2OE3zUA1zH"
      },
      "outputs": [],
      "source": [
        "def tune_random_forest(dataset, target_column_name):\n",
        "    # Split the data into features (x) and the target variable (y)\n",
        "    x = dataset.drop(columns=[target_column_name])\n",
        "    y = dataset[target_column_name]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a RandomForestClassifier\n",
        "    classifier = RandomForestClassifier()\n",
        "\n",
        "    # Define a parameter grid for tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [25, 50, 100],\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 5, 10, 15],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Create a GridSearchCV object with 5-fold cross-validation\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Get the best parameters and estimator from the grid search\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    # Assess the model using 5-fold cross-validation and print accuracy and F1 scores\n",
        "    accuracy_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Cross-Validation Accuracy:\", round(np.mean(accuracy_scores),3))\n",
        "    print(\"Cross-Validation F1 Score:\", round(np.mean(f1_scores),3))\n",
        "\n",
        "    # Fit the best model on the entire training set\n",
        "    best_estimator.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = best_estimator.predict(x_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Test Set Accuracy:\", round(test_accuracy,3))\n",
        "    print(\"Test Set F1 Score:\", round(test_f1_score,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSW5euHmA9tq"
      },
      "outputs": [],
      "source": [
        "tune_random_forest(heart_df, 'output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQXCXCBBStu"
      },
      "outputs": [],
      "source": [
        "tune_random_forest(stroke_df, 'stroke')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnXFgIhtBWi6"
      },
      "outputs": [],
      "source": [
        "tune_random_forest(water_df, 'Potability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqC1Dn1NI8Mv"
      },
      "source": [
        "# **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5QVQXhCTy_h"
      },
      "outputs": [],
      "source": [
        "def tune_logistic_regression(dataset, target_column_name):\n",
        "    # Split the data into features (x) and the target variable (y)\n",
        "    x = dataset.drop(columns=[target_column_name])\n",
        "    y = dataset[target_column_name]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Scaling the data\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.fit_transform(x_test)\n",
        "\n",
        "    # Create a LogisticRegression classifier\n",
        "    classifier = LogisticRegression()\n",
        "\n",
        "    # Define a parameter grid for tuning\n",
        "    param_grid = {\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "        'max_iter': [500, 750, 1000]\n",
        "    }\n",
        "\n",
        "    # Create a GridSearchCV object with 5-fold cross-validation\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Get the best parameters and estimator from the grid search\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    # Assess the model using 5-fold cross-validation and print accuracy and F1 scores\n",
        "    accuracy_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Cross-Validation Accuracy:\", round(np.mean(accuracy_scores),3))\n",
        "    print(\"Cross-Validation F1 Score:\", round(np.mean(f1_scores),3))\n",
        "\n",
        "    # Fit the best model on the entire training set\n",
        "    best_estimator.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = best_estimator.predict(x_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Test Set Accuracy:\", round(test_accuracy,3))\n",
        "    print(\"Test Set F1 Score:\", round(test_f1_score,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKDtZQ3KYhqz"
      },
      "outputs": [],
      "source": [
        "tune_logistic_regression(heart_df, 'output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnHnBWaXYkq4"
      },
      "outputs": [],
      "source": [
        "tune_logistic_regression(stroke_df, 'stroke')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh7P5hyFYqGu"
      },
      "outputs": [],
      "source": [
        "tune_logistic_regression(water_df, 'Potability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv69a9zggW1x"
      },
      "source": [
        "# **k-NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoSsARPkgduQ"
      },
      "outputs": [],
      "source": [
        "def tune_knn(dataset, target_column_name, scaling=True, k_values=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]):\n",
        "    # Split the data into features (x) and the target variable (y)\n",
        "    x = dataset.drop(columns=[target_column_name])\n",
        "    y = dataset[target_column_name]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Scaling the data\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.fit_transform(x_test)\n",
        "\n",
        "    # Create a KNeighborsClassifier\n",
        "    classifier = KNeighborsClassifier()\n",
        "\n",
        "    # Define a parameter grid for tuning\n",
        "    param_grid = {\n",
        "        'n_neighbors': k_values,  # Try different k values\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "        'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "    }\n",
        "\n",
        "    # Create a GridSearchCV object with 5-fold cross-validation\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Get the best parameters and estimator from the grid search\n",
        "    best_params = grid_search.best_params_\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "\n",
        "    # Assess the model using 5-fold cross-validation and print accuracy and F1 scores\n",
        "    accuracy_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(best_estimator, x_train, y_train, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Cross-Validation Accuracy:\", round(np.mean(accuracy_scores),3))\n",
        "    print(\"Cross-Validation F1 Score:\", round(np.mean(f1_scores),3))\n",
        "\n",
        "    # Fit the best model on the entire training set\n",
        "    best_estimator.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = best_estimator.predict(x_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Test Set Accuracy:\", round(test_accuracy,3))\n",
        "    print(\"Test Set F1 Score:\", round(test_f1_score,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjBRH-D6iI_T"
      },
      "outputs": [],
      "source": [
        "tune_knn(heart_df, 'output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSC6h-9xiKgQ"
      },
      "outputs": [],
      "source": [
        "tune_knn(stroke_df, 'stroke')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Uwh_bTUiMC8"
      },
      "outputs": [],
      "source": [
        "tune_knn(water_df, 'Potability')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FCA Preprocessing**"
      ],
      "metadata": {
        "id": "X2zH46qKM9Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for FCA binarization\n",
        "heart_df_fca = heart_df.copy()"
      ],
      "metadata": {
        "id": "noOhsIILNG41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarizing age based on position around the mean\n",
        "heart_df_fca['age'] = (heart_df_fca['age'] >= heart_df['age'].mean()).astype(int)\n",
        "\n",
        "# Binarizing chest pain type (cp) whether or not they belong to types (0) (heart related) or (1,2,3) (non heart related)\n",
        "heart_df_fca['cp'] = heart_df_fca['cp'].apply(lambda x: 1 if x in [0] else 0)\n",
        "\n",
        "# Binarizing resting blood pressure (trtbps) based on its position around normal blood pressure of humans being 120 mm/Hg\n",
        "heart_df_fca['trtbps'] = (heart_df_fca['trtbps'] > 120).astype(int)\n",
        "\n",
        "# Binarizing cholestrol (chol) based on its position around normal cholestrol level which is under 200 mg/dL\n",
        "heart_df_fca['chol'] = (heart_df_fca['chol'] >= 200).astype(int)\n",
        "\n",
        "# Binarizing resting electrocardiographic results (restecg) based on whether it is normal (1) or not (0,2)\n",
        "heart_df_fca['restecg'] = heart_df_fca['restecg'].apply(lambda x: 1 if x in [0, 2] else 0)\n",
        "\n",
        "# Binarizing maximum heart rate achieved (thalachh) based on theoretical max heart rate calculated by (220-age)\n",
        "heart_df_fca['thalachh'] = (heart_df_fca['thalachh'] > (220-heart_df['age'])).astype(int)\n",
        "\n",
        "# Binarizing oldpeak based on position around the mean\n",
        "heart_df_fca['oldpeak'] = (heart_df_fca['oldpeak'] >= heart_df['oldpeak'].mean()).astype(int)\n",
        "\n",
        "# Binarizing slope based on flat/positive (1,2) slope and negative slope (0)\n",
        "heart_df_fca['slp'] = heart_df_fca['slp'].apply(lambda x: 0 if x in [1, 2] else 1)\n",
        "\n",
        "# Binarizing number of major vessels based on whether there are any or not\n",
        "heart_df_fca['caa'] = heart_df_fca['caa'].apply(lambda x: 0 if x in [1, 2, 3] else 1)\n",
        "\n",
        "# Binarizing thalassemia based on whether there is normal bloodflow (2) or not (0,1,3)\n",
        "heart_df_fca['thall'] = heart_df_fca['thall'].apply(lambda x: 1 if x in [0, 1, 3] else 0)"
      ],
      "metadata": {
        "id": "Q27xw9rdNgB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_df_fca.head()"
      ],
      "metadata": {
        "id": "kKub0GwmXXjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for FCA binarization\n",
        "stroke_df_fca = stroke_df.copy()"
      ],
      "metadata": {
        "id": "4nyq-YaplmuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarizing age based on position around the mean\n",
        "stroke_df_fca['age'] = (stroke_df_fca['age'] >= stroke_df['age'].mean()).astype(int)\n",
        "\n",
        "# Binarizing average glucose level based on position around the mean\n",
        "stroke_df_fca['avg_glucose_level'] = (stroke_df_fca['avg_glucose_level'] > stroke_df['avg_glucose_level'].mean()).astype(int)\n",
        "\n",
        "# Binarizing bmi around the max normal limit 24.9\n",
        "stroke_df_fca['bmi'] = (stroke_df_fca['bmi'] > 24.9).astype(int)"
      ],
      "metadata": {
        "id": "DgVavc8VXW82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stroke_df_fca.head()"
      ],
      "metadata": {
        "id": "FnmYfmWRqotE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for FCA binarization\n",
        "water_df_fca = water_df.copy()"
      ],
      "metadata": {
        "id": "5cranyQzvPDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarizing pH based on pH range of drinking water between 6.5 and 8.5\n",
        "water_df_fca['ph'] = (water_df['ph'] > 6.5) & (water_df['ph'] < 8.5)\n",
        "water_df_fca['ph'] = water_df_fca['ph'].astype(int)\n",
        "\n",
        "# Binarizing Hardness based on position around the mean\n",
        "water_df_fca['Hardness'] = (water_df_fca['Hardness'] < water_df['Hardness'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Solids based on position around the mean\n",
        "water_df_fca['Solids'] = (water_df_fca['Solids'] < water_df['Solids'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Chloramines based on position around the mean\n",
        "water_df_fca['Chloramines'] = (water_df_fca['Chloramines'] < water_df['Chloramines'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Sulfate based on position around the mean\n",
        "water_df_fca['Sulfate'] = (water_df_fca['Sulfate'] < water_df['Sulfate'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Conductivity based on position around the mean\n",
        "water_df_fca['Conductivity'] = (water_df_fca['Conductivity'] < water_df['Conductivity'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Organic Carbon amount based on position around the mean\n",
        "water_df_fca['Organic_carbon'] = (water_df_fca['Organic_carbon'] < water_df['Organic_carbon'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Trihalomethanes based on position around the mean\n",
        "water_df_fca['Trihalomethanes'] = (water_df_fca['Trihalomethanes'] < water_df['Trihalomethanes'].mean()).astype(int)\n",
        "\n",
        "# Binarizing Turbidity based on position around the mean\n",
        "water_df_fca['Turbidity'] = (water_df_fca['Turbidity'] < water_df['Turbidity'].mean()).astype(int)"
      ],
      "metadata": {
        "id": "QXdPLqHnzYyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "water_df_fca.head()"
      ],
      "metadata": {
        "id": "2yh5LhQ4kVJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FCA Lazy Binary Classification**"
      ],
      "metadata": {
        "id": "5jEVPK_mW-_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_LazyBinaryClassifier(dataset, target_column_name, method='standard'):\n",
        "  # Define the number of folds for cross-validation\n",
        "  num_folds = 5\n",
        "  kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "  # Split the data into features (x) and the target variable (y)\n",
        "  x = dataset.drop(columns=[target_column_name])\n",
        "  y = dataset[target_column_name]\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Initialize lists to store results\n",
        "  accuracies = []\n",
        "  weighted_f1_scores = []\n",
        "\n",
        "  # Perform k-fold cross-validation\n",
        "  for train_index, val_index in kf.split(x_tr, y_tr):\n",
        "    x_train, x_val = x_tr.iloc[train_index], x_tr.iloc[val_index]\n",
        "    y_train, y_val = y_tr.iloc[train_index], y_tr.iloc[val_index]\n",
        "\n",
        "    # Fit the classifier on the training set\n",
        "    bin_cls = fcalc.classifier.BinarizedBinaryClassifier(x_train.values, y_train.values, method=method)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    bin_cls.predict(x_val.values)\n",
        "\n",
        "    # Calculate accuracy and weighted F1 score\n",
        "    accuracy = accuracy_score(y_val, bin_cls.predictions)\n",
        "    weighted_f1 = f1_score(y_val, bin_cls.predictions, average='weighted')\n",
        "\n",
        "    # Append results to lists\n",
        "    accuracies.append(accuracy)\n",
        "    weighted_f1_scores.append(weighted_f1)\n",
        "\n",
        "  # Calculate and print average metrics over all folds\n",
        "  average_accuracy = sum(accuracies) / num_folds\n",
        "  average_weighted_f1 = sum(weighted_f1_scores) / num_folds\n",
        "\n",
        "  print(\"Cross-Validation Accuracy:\", round(average_accuracy, 3))\n",
        "  print(\"Cross-Validation F1 Score:\", round(average_weighted_f1, 3))\n",
        "\n",
        "  # Predict on a separate test set\n",
        "  test_predictions = bin_cls.predict(x_ts.values)\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  test_accuracy = accuracy_score(y_ts, bin_cls.predictions)\n",
        "  test_f1_score = f1_score(y_ts, bin_cls.predictions, average='weighted')\n",
        "\n",
        "  print(\"Test Set Accuracy:\", round(test_accuracy, 3))\n",
        "  print(\"Test Set F1 Score:\", round(test_f1_score, 3))"
      ],
      "metadata": {
        "id": "THUxHyFtwjs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyBinaryClassifier(heart_df_fca, 'output')"
      ],
      "metadata": {
        "id": "jf7Rs0PlW6ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyBinaryClassifier(stroke_df_fca.iloc[0:2000], 'stroke', method='ratio-support')"
      ],
      "metadata": {
        "id": "jJdyT2MhZD-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyBinaryClassifier(water_df_fca.iloc[500:4000], 'Potability', method='ratio-support')"
      ],
      "metadata": {
        "id": "zfkr7nQQZasy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FCA Pattern Classifier**"
      ],
      "metadata": {
        "id": "7fHCZypzAy8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_LazyPatternClassifier(dataset, target_column_name, cat_list=None, method='standard'):\n",
        "  # Define the number of folds for cross-validation\n",
        "  num_folds = 5\n",
        "  kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "  # Split the data into features (x) and the target variable (y)\n",
        "  x = dataset.drop(columns=[target_column_name])\n",
        "  y = dataset[target_column_name]\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Initialize lists to store results\n",
        "  accuracies = []\n",
        "  weighted_f1_scores = []\n",
        "\n",
        "  # Perform k-fold cross-validation\n",
        "  for train_index, val_index in kf.split(x_tr, y_tr):\n",
        "    x_train, x_val = x_tr.iloc[train_index], x_tr.iloc[val_index]\n",
        "    y_train, y_val = y_tr.iloc[train_index], y_tr.iloc[val_index]\n",
        "\n",
        "    # Fit the classifier on the training set\n",
        "    pat_cls = fcalc.classifier.PatternBinaryClassifier(x_train.values, y_train.to_numpy(), categorical=cat_list, method=method)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    pat_cls.predict(x_val.values)\n",
        "\n",
        "    # Calculate accuracy and weighted F1 score\n",
        "    accuracy = accuracy_score(y_val, pat_cls.predictions)\n",
        "    weighted_f1 = f1_score(y_val, pat_cls.predictions, average='weighted')\n",
        "\n",
        "    # Append results to lists\n",
        "    accuracies.append(accuracy)\n",
        "    weighted_f1_scores.append(weighted_f1)\n",
        "\n",
        "  # Calculate and print average metrics over all folds\n",
        "  average_accuracy = sum(accuracies) / num_folds\n",
        "  average_weighted_f1 = sum(weighted_f1_scores) / num_folds\n",
        "\n",
        "  print(\"Cross-Validation Accuracy:\", round(average_accuracy, 3))\n",
        "  print(\"Cross-Validation F1 Score:\", round(average_weighted_f1, 3))\n",
        "\n",
        "  pat_cls = fcalc.classifier.PatternBinaryClassifier(x_tr.values, y_tr.to_numpy(), categorical=cat_list, method=method)\n",
        "\n",
        "  # Predict on a separate test set\n",
        "  pat_cls.predict(x_ts.values)\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  test_accuracy = accuracy_score(y_ts, pat_cls.predictions)\n",
        "  test_f1_score = f1_score(y_ts, pat_cls.predictions, average='weighted')\n",
        "\n",
        "  print(\"Test Set Accuracy:\", round(test_accuracy, 3))\n",
        "  print(\"Test Set F1 Score:\", round(test_f1_score, 3))"
      ],
      "metadata": {
        "id": "1qL6_y39A4bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyPatternClassifier(heart_df, 'output', np.array([1,2,5,6,8,10,11,12]), method='ratio-support')"
      ],
      "metadata": {
        "id": "gSaSJJPgLCYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyPatternClassifier(stroke_df.iloc[0:1000], 'stroke', np.array([1,2,3,4,7,8,9,10,11,12,13,14,15,16,17,18]), method='standard-support')"
      ],
      "metadata": {
        "id": "Z2nRd0z0hFKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_LazyPatternClassifier(water_df.iloc[500:1500], 'Potability', method='standard')"
      ],
      "metadata": {
        "id": "vJ8vnOUgZtsY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YfiNNtgrCnzB",
        "KpNoLU7hIx3T",
        "BZPpMn7pI0jY",
        "MqC1Dn1NI8Mv",
        "Lv69a9zggW1x",
        "X2zH46qKM9Q9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}